{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.10.0-cp310-cp310-win_amd64.whl (455.9 MB)\n",
      "     ------------------------------------ 455.9/455.9 MB 462.9 kB/s eta 0:00:00\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp310-cp310-win_amd64.whl (895 kB)\n",
      "     -------------------------------------- 895.7/895.7 kB 5.6 MB/s eta 0:00:00\n",
      "Collecting tensorflow-estimator<2.11,>=2.10.0\n",
      "  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
      "     -------------------------------------- 438.7/438.7 kB 9.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in c:\\users\\i538420\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (58.1.0)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.0.1-py3-none-any.whl (5.4 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\i538420\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (21.3)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.50.0-cp310-cp310-win_amd64.whl (3.6 MB)\n",
      "     ---------------------------------------- 3.6/3.6 MB 1.4 MB/s eta 0:00:00\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     ---------------------------------------- 57.5/57.5 kB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\users\\i538420\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp310-cp310-win_amd64.whl (35 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\i538420\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (1.23.2)\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.7.0-cp310-cp310-win_amd64.whl (2.6 MB)\n",
      "     ---------------------------------------- 2.6/2.6 MB 2.5 MB/s eta 0:00:00\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.3.0-py3-none-any.whl (124 kB)\n",
      "     -------------------------------------- 124.6/124.6 kB 1.8 MB/s eta 0:00:00\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-22.9.24-py2.py3-none-any.whl (26 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     ---------------------------------------- 65.5/65.5 kB 1.2 MB/s eta 0:00:00\n",
      "Collecting tensorboard<2.11,>=2.10\n",
      "  Downloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
      "     ---------------------------------------- 5.9/5.9 MB 1.3 MB/s eta 0:00:00\n",
      "Collecting typing-extensions>=3.6.6\n",
      "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.27.0-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 3.6 MB/s eta 0:00:00\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "     ---------------------------------------- 42.6/42.6 kB 2.0 MB/s eta 0:00:00\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-14.0.6-py2.py3-none-win_amd64.whl (14.2 MB)\n",
      "     ---------------------------------------- 14.2/14.2 MB 1.1 MB/s eta 0:00:00\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\i538420\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Collecting wheel<1.0,>=0.23.0\n",
      "  Downloading wheel-0.37.1-py2.py3-none-any.whl (35 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "     -------------------------------------- 781.3/781.3 kB 4.9 MB/s eta 0:00:00\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "     ---------------------------------------- 93.3/93.3 kB 2.7 MB/s eta 0:00:00\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
      "     -------------------------------------- 232.7/232.7 kB 2.9 MB/s eta 0:00:00\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.13.0-py2.py3-none-any.whl (174 kB)\n",
      "     -------------------------------------- 174.5/174.5 kB 2.1 MB/s eta 0:00:00\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "     ---------------------------------------- 62.8/62.8 kB 1.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\i538420\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging->tensorflow) (3.0.9)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "     -------------------------------------- 155.3/155.3 kB 1.6 MB/s eta 0:00:00\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting charset-normalizer<3,>=2\n",
      "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
      "     -------------------------------------- 140.4/140.4 kB 4.2 MB/s eta 0:00:00\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "     -------------------------------------- 61.5/61.5 kB 657.7 kB/s eta 0:00:00\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2022.9.24-py3-none-any.whl (161 kB)\n",
      "     -------------------------------------- 161.1/161.1 kB 1.6 MB/s eta 0:00:00\n",
      "Collecting MarkupSafe>=2.1.1\n",
      "  Downloading MarkupSafe-2.1.1-cp310-cp310-win_amd64.whl (17 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "     ---------------------------------------- 77.1/77.1 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     -------------------------------------- 151.7/151.7 kB 3.0 MB/s eta 0:00:00\n",
      "Installing collected packages: tensorboard-plugin-wit, pyasn1, libclang, flatbuffers, wrapt, wheel, urllib3, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, pyasn1-modules, protobuf, opt-einsum, oauthlib, MarkupSafe, markdown, keras-preprocessing, idna, h5py, grpcio, google-pasta, gast, charset-normalizer, certifi, cachetools, absl-py, werkzeug, requests, google-auth, astunparse, requests-oauthlib, google-auth-oauthlib, tensorboard, tensorflow\n",
      "Successfully installed MarkupSafe-2.1.1 absl-py-1.3.0 astunparse-1.6.3 cachetools-5.2.0 certifi-2022.9.24 charset-normalizer-2.1.1 flatbuffers-22.9.24 gast-0.4.0 google-auth-2.13.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.50.0 h5py-3.7.0 idna-3.4 keras-preprocessing-1.1.2 libclang-14.0.6 markdown-3.4.1 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-3.19.6 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.28.1 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.10.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.10.0 tensorflow-estimator-2.10.0 tensorflow-io-gcs-filesystem-0.27.0 termcolor-2.0.1 typing-extensions-4.4.0 urllib3-1.26.12 werkzeug-2.2.2 wheel-0.37.1 wrapt-1.14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script wheel.exe is installed in 'c:\\Users\\I538420\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts pyrsa-decrypt.exe, pyrsa-encrypt.exe, pyrsa-keygen.exe, pyrsa-priv2pub.exe, pyrsa-sign.exe and pyrsa-verify.exe are installed in 'c:\\Users\\I538420\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script markdown_py.exe is installed in 'c:\\Users\\I538420\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script normalizer.exe is installed in 'c:\\Users\\I538420\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script google-oauthlib-tool.exe is installed in 'c:\\Users\\I538420\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script tensorboard.exe is installed in 'c:\\Users\\I538420\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts estimator_ckpt_converter.exe, import_pb_to_tensorboard.exe, saved_model_cli.exe, tensorboard.exe, tf_upgrade_v2.exe, tflite_convert.exe, toco.exe and toco_from_protos.exe are installed in 'c:\\Users\\I538420\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from keras.layers import Dense,Dropout,SimpleRNN,LSTM\n",
    "from keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Bought</th>\n",
       "      <th>Sold</th>\n",
       "      <th>NV</th>\n",
       "      <th>Lost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>735422368.7</td>\n",
       "      <td>712147000.0</td>\n",
       "      <td>403465.600</td>\n",
       "      <td>2.287190e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-02-01</td>\n",
       "      <td>657988856.3</td>\n",
       "      <td>640633717.3</td>\n",
       "      <td>347800.550</td>\n",
       "      <td>1.700734e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-03-01</td>\n",
       "      <td>663373030.9</td>\n",
       "      <td>649168952.2</td>\n",
       "      <td>357794.550</td>\n",
       "      <td>1.384628e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-04-01</td>\n",
       "      <td>533983791.4</td>\n",
       "      <td>519744979.3</td>\n",
       "      <td>255663.800</td>\n",
       "      <td>1.398315e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-05-01</td>\n",
       "      <td>493381739.3</td>\n",
       "      <td>481595212.5</td>\n",
       "      <td>196548.400</td>\n",
       "      <td>1.158998e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>2021-10-01</td>\n",
       "      <td>514226362.7</td>\n",
       "      <td>506857003.8</td>\n",
       "      <td>316669.565</td>\n",
       "      <td>7.052689e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>2021-11-01</td>\n",
       "      <td>541704980.8</td>\n",
       "      <td>532236963.9</td>\n",
       "      <td>356965.930</td>\n",
       "      <td>9.111051e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>2021-12-01</td>\n",
       "      <td>705779288.1</td>\n",
       "      <td>696793547.9</td>\n",
       "      <td>462805.350</td>\n",
       "      <td>8.522935e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>743431196.2</td>\n",
       "      <td>731220808.8</td>\n",
       "      <td>490604.900</td>\n",
       "      <td>1.171978e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>2022-02-01</td>\n",
       "      <td>629630679.8</td>\n",
       "      <td>621472166.7</td>\n",
       "      <td>421857.350</td>\n",
       "      <td>7.736656e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date       Bought         Sold          NV          Lost\n",
       "0    2013-01-01  735422368.7  712147000.0  403465.600  2.287190e+07\n",
       "1    2013-02-01  657988856.3  640633717.3  347800.550  1.700734e+07\n",
       "2    2013-03-01  663373030.9  649168952.2  357794.550  1.384628e+07\n",
       "3    2013-04-01  533983791.4  519744979.3  255663.800  1.398315e+07\n",
       "4    2013-05-01  493381739.3  481595212.5  196548.400  1.158998e+07\n",
       "..          ...          ...          ...         ...           ...\n",
       "105  2021-10-01  514226362.7  506857003.8  316669.565  7.052689e+06\n",
       "106  2021-11-01  541704980.8  532236963.9  356965.930  9.111051e+06\n",
       "107  2021-12-01  705779288.1  696793547.9  462805.350  8.522935e+06\n",
       "108  2022-01-01  743431196.2  731220808.8  490604.900  1.171978e+07\n",
       "109  2022-02-01  629630679.8  621472166.7  421857.350  7.736656e+06\n",
       "\n",
       "[110 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"data.csv\")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "Bought\n",
      "Sold\n",
      "NV\n",
      "Lost\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sold</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-01</th>\n",
       "      <td>712147000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-01</th>\n",
       "      <td>640633717.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-01</th>\n",
       "      <td>649168952.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-04-01</th>\n",
       "      <td>519744979.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-05-01</th>\n",
       "      <td>481595212.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Sold\n",
       "Date                   \n",
       "2013-01-01  712147000.0\n",
       "2013-02-01  640633717.3\n",
       "2013-03-01  649168952.2\n",
       "2013-04-01  519744979.3\n",
       "2013-05-01  481595212.5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in df.columns:\n",
    "    print(col)\n",
    "\n",
    "df.columns = df.columns.str.strip()\n",
    "#df['Month']=df['Month'].str.strip()\n",
    "#df['Date'] = pd.to_datetime(['{}-{}-01'.format(y, m)for y, m in zip(df.Year, df.Month)])\n",
    "#pd.to_datetime(df[['Year', 'Month']].assign(DAY=1))\n",
    "\n",
    "#for time series analysis date must be set as index\n",
    "df = df.set_index('Date')\n",
    "df1 = df.filter(['Bought', 'Sold', 'NV', 'Lost'], axis=1)\n",
    "df1.to_csv('data.csv')\n",
    "df1 = df.filter(['Sold'], axis=1)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2matrix(data_arr, look_back):\n",
    " X, Y = [], []\n",
    " for i in range(len(data_arr)-look_back):\n",
    "  d = i+look_back\n",
    "  X.append(data_arr[i:d, 0])\n",
    "  Y.append(data_arr[d, 0])\n",
    " return np.array(X), np.array(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('data.csv')\n",
    "df1 = df.filter(['Sold'], axis=1)\n",
    "#Split data set into testing dataset and train dataset\n",
    "train_size = 90\n",
    "train, test = df1.values[0:train_size,\n",
    "                         :], df1.values[train_size:len(df1.values), :]\n",
    "# setup look_back window\n",
    "look_back = 12\n",
    "#convert dataset into right shape in order to input into the DNN\n",
    "trainX, trainY = convert2matrix(train, look_back)\n",
    "testX, testY = convert2matrix(test, look_back)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "def model_dnn(look_back):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=32, input_dim=look_back, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer='adam', metrics=['mse', 'mae'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 110 entries, 2013-01-01 to 2022-02-01\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Sold    110 non-null    float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 1.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df1.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 78 samples, validate on 8 samples\n",
      "Epoch 1/100\n",
      "78/78 [==============================] - 1s 7ms/step - loss: 231524200463587488.0000 - mse: 231524215000399872.0000 - mae: 474959648.0000 - val_loss: 253908914532253696.0000 - val_mse: 253908914532253696.0000 - val_mae: 498101408.0000\n",
      "Epoch 2/100\n",
      "78/78 [==============================] - 0s 114us/step - loss: 175189998016921600.0000 - mse: 175190015196790784.0000 - mae: 410934400.0000 - val_loss: 177506737736122368.0000 - val_mse: 177506737736122368.0000 - val_mae: 414086432.0000\n",
      "Epoch 3/100\n",
      "78/78 [==============================] - 0s 119us/step - loss: 118955352912177008.0000 - mse: 118955355555233792.0000 - mae: 334941152.0000 - val_loss: 111495023131885568.0000 - val_mse: 111495023131885568.0000 - val_mae: 324270848.0000\n",
      "Epoch 4/100\n",
      "78/78 [==============================] - 0s 265us/step - loss: 70382956224446464.0000 - mse: 70382951929479168.0000 - mae: 251760960.0000 - val_loss: 62103530588078080.0000 - val_mse: 62103530588078080.0000 - val_mae: 235372000.0000\n",
      "Epoch 5/100\n",
      "78/78 [==============================] - 0s 259us/step - loss: 35557519838654780.0000 - mse: 35557523142475776.0000 - mae: 168891504.0000 - val_loss: 28314376477868032.0000 - val_mse: 28314376477868032.0000 - val_mae: 149094256.0000\n",
      "Epoch 6/100\n",
      "78/78 [==============================] - 0s 175us/step - loss: 14743507604986014.0000 - mse: 14743508183154688.0000 - mae: 94897096.0000 - val_loss: 10930771225214976.0000 - val_mse: 10930771225214976.0000 - val_mae: 88013808.0000\n",
      "Epoch 7/100\n",
      "78/78 [==============================] - 0s 164us/step - loss: 7110556853106216.0000 - mse: 7110556729212928.0000 - mae: 68044032.0000 - val_loss: 6507170732441600.0000 - val_mse: 6507170732441600.0000 - val_mae: 62832688.0000\n",
      "Epoch 8/100\n",
      "78/78 [==============================] - 0s 175us/step - loss: 8810275086814917.0000 - mse: 8810275541090304.0000 - mae: 80754000.0000 - val_loss: 8665110478323712.0000 - val_mse: 8665110478323712.0000 - val_mae: 76914832.0000\n",
      "Epoch 9/100\n",
      "78/78 [==============================] - 0s 441us/step - loss: 13162365613042768.0000 - mse: 13162365365256192.0000 - mae: 98634512.0000 - val_loss: 10589435141816320.0000 - val_mse: 10589435141816320.0000 - val_mae: 85454296.0000\n",
      "Epoch 10/100\n",
      "78/78 [==============================] - 0s 170us/step - loss: 14927063687203288.0000 - mse: 14927063274225664.0000 - mae: 105005224.0000 - val_loss: 9905042840616960.0000 - val_mse: 9905042840616960.0000 - val_mae: 83102944.0000\n",
      "Epoch 11/100\n",
      "78/78 [==============================] - 0s 167us/step - loss: 13310980154014482.0000 - mse: 13310979823632384.0000 - mae: 99044216.0000 - val_loss: 7870445965541376.0000 - val_mse: 7870445965541376.0000 - val_mae: 73678248.0000\n",
      "Epoch 12/100\n",
      "78/78 [==============================] - 0s 112us/step - loss: 10288817371974104.0000 - mse: 10288817495867392.0000 - mae: 86932336.0000 - val_loss: 6389409171636224.0000 - val_mse: 6389409171636224.0000 - val_mae: 63651400.0000\n",
      "Epoch 13/100\n",
      "78/78 [==============================] - 0s 269us/step - loss: 7744203248450324.0000 - mse: 7744202918068224.0000 - mae: 74806784.0000 - val_loss: 6403990585606144.0000 - val_mse: 6403990585606144.0000 - val_mae: 57117640.0000\n",
      "Epoch 14/100\n",
      "78/78 [==============================] - 0s 171us/step - loss: 6425737574340765.0000 - mse: 6425737615638528.0000 - mae: 66641408.0000 - val_loss: 7419277367836672.0000 - val_mse: 7419277367836672.0000 - val_mae: 63356364.0000\n",
      "Epoch 15/100\n",
      "78/78 [==============================] - 0s 132us/step - loss: 6155382639639316.0000 - mse: 6155382309257216.0000 - mae: 62893600.0000 - val_loss: 8640025184960512.0000 - val_mse: 8640025184960512.0000 - val_mae: 74524152.0000\n",
      "Epoch 16/100\n",
      "78/78 [==============================] - 0s 181us/step - loss: 6338213863529865.0000 - mse: 6338213698338816.0000 - mae: 62029612.0000 - val_loss: 9426100300021760.0000 - val_mse: 9426100300021760.0000 - val_mae: 80161392.0000\n",
      "Epoch 17/100\n",
      "78/78 [==============================] - 0s 166us/step - loss: 6470604908900667.0000 - mse: 6470604991496192.0000 - mae: 62457580.0000 - val_loss: 9532241826807808.0000 - val_mse: 9532241826807808.0000 - val_mae: 81176152.0000\n",
      "Epoch 18/100\n",
      "78/78 [==============================] - 0s 171us/step - loss: 6354457388545576.0000 - mse: 6354456727781376.0000 - mae: 62069052.0000 - val_loss: 9021335065853952.0000 - val_mse: 9021335065853952.0000 - val_mae: 78434112.0000\n",
      "Epoch 19/100\n",
      "78/78 [==============================] - 0s 138us/step - loss: 6044366366270071.0000 - mse: 6044365994590208.0000 - mae: 60878264.0000 - val_loss: 8179690053304320.0000 - val_mse: 8179690053304320.0000 - val_mae: 73294280.0000\n",
      "Epoch 20/100\n",
      "78/78 [==============================] - 0s 191us/step - loss: 5701478643340209.0000 - mse: 5701478891126784.0000 - mae: 59806424.0000 - val_loss: 7340137763569664.0000 - val_mse: 7340137763569664.0000 - val_mae: 67231560.0000\n",
      "Epoch 21/100\n",
      "78/78 [==============================] - 0s 198us/step - loss: 5469962106750188.0000 - mse: 5469961900261376.0000 - mae: 59671808.0000 - val_loss: 6672052379451392.0000 - val_mse: 6672052379451392.0000 - val_mae: 61306256.0000\n",
      "Epoch 22/100\n",
      "78/78 [==============================] - 0s 259us/step - loss: 5393243831594063.0000 - mse: 5393244120678400.0000 - mae: 60104532.0000 - val_loss: 6244010570022912.0000 - val_mse: 6244010570022912.0000 - val_mae: 58613764.0000\n",
      "Epoch 23/100\n",
      "78/78 [==============================] - 0s 234us/step - loss: 5394340947951616.0000 - mse: 5394340411080704.0000 - mae: 60642496.0000 - val_loss: 6027031506583552.0000 - val_mse: 6027031506583552.0000 - val_mae: 57242864.0000\n",
      "Epoch 24/100\n",
      "78/78 [==============================] - 0s 260us/step - loss: 5397408917427121.0000 - mse: 5397409165213696.0000 - mae: 60916224.0000 - val_loss: 5973894171197440.0000 - val_mse: 5973894171197440.0000 - val_mae: 57192768.0000\n",
      "Epoch 25/100\n",
      "78/78 [==============================] - 0s 125us/step - loss: 5358539298207271.0000 - mse: 5358539711184896.0000 - mae: 60663828.0000 - val_loss: 6037854287298560.0000 - val_mse: 6037854287298560.0000 - val_mae: 58190076.0000\n",
      "Epoch 26/100\n",
      "78/78 [==============================] - 0s 182us/step - loss: 5282315026885711.0000 - mse: 5282315315970048.0000 - mae: 60049532.0000 - val_loss: 6188650589061120.0000 - val_mse: 6188650589061120.0000 - val_mae: 59822376.0000\n",
      "Epoch 27/100\n",
      "78/78 [==============================] - 0s 190us/step - loss: 5200254638368611.0000 - mse: 5200254597070848.0000 - mae: 59300044.0000 - val_loss: 6384022208905216.0000 - val_mse: 6384022208905216.0000 - val_mae: 61615296.0000\n",
      "Epoch 28/100\n",
      "78/78 [==============================] - 0s 143us/step - loss: 5136740538102075.0000 - mse: 5136740620697600.0000 - mae: 58687948.0000 - val_loss: 6568370795184128.0000 - val_mse: 6568370795184128.0000 - val_mae: 63164108.0000\n",
      "Epoch 29/100\n",
      "78/78 [==============================] - 0s 230us/step - loss: 5091965710530087.0000 - mse: 5091965586636800.0000 - mae: 58226996.0000 - val_loss: 6689700936941568.0000 - val_mse: 6689700936941568.0000 - val_mae: 64217032.0000\n",
      "Epoch 30/100\n",
      "78/78 [==============================] - 0s 167us/step - loss: 5054044659616217.0000 - mse: 5054044783509504.0000 - mae: 57873716.0000 - val_loss: 6721712401940480.0000 - val_mse: 6721712401940480.0000 - val_mae: 64691816.0000\n",
      "Epoch 31/100\n",
      "78/78 [==============================] - 0s 188us/step - loss: 5012321509552443.0000 - mse: 5012321323712512.0000 - mae: 57616996.0000 - val_loss: 6670085821300736.0000 - val_mse: 6670085821300736.0000 - val_mae: 64654728.0000\n",
      "Epoch 32/100\n",
      "78/78 [==============================] - 0s 156us/step - loss: 4965642049824611.0000 - mse: 4965642008526848.0000 - mae: 57443044.0000 - val_loss: 6564469354266624.0000 - val_mse: 6564469354266624.0000 - val_mae: 64275972.0000\n",
      "Epoch 33/100\n",
      "78/78 [==============================] - 0s 143us/step - loss: 4919318762649127.0000 - mse: 4919318638755840.0000 - mae: 57318268.0000 - val_loss: 6443127401349120.0000 - val_mse: 6443127401349120.0000 - val_mae: 63765164.0000\n",
      "Epoch 34/100\n",
      "78/78 [==============================] - 0s 124us/step - loss: 4877826714864089.0000 - mse: 4877827107192832.0000 - mae: 57209508.0000 - val_loss: 6335381167407104.0000 - val_mse: 6335381167407104.0000 - val_mae: 63305068.0000\n"
     ]
    }
   ],
   "source": [
    "model=model_dnn(look_back)\n",
    "history=model.fit(trainX,trainY, epochs=100, batch_size=30, verbose=1, validation_data=(testX,testY),callbacks=[EarlyStopping(monitor='val_loss', patience=10)],shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(history):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Test Loss')\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Root Mean Squared Error(RMSE): 69660310.44; Train Mean Absolute Error(MAE) : 4852558740848640.00 \n",
      "Test Root Mean Squared Error(RMSE): 79595107.69; Test Mean Absolute Error(MAE) : 6335381167407104.00 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_score = model.evaluate(trainX, trainY, verbose=0)\n",
    "print('Train Root Mean Squared Error(RMSE): %.2f; Train Mean Absolute Error(MAE) : %.2f '\n",
    "      % (np.sqrt(train_score[0]), train_score[1]))\n",
    "test_score = model.evaluate(testX, testY, verbose=0)\n",
    "print('Test Root Mean Squared Error(RMSE): %.2f; Test Mean Absolute Error(MAE) : %.2f '\n",
    "      % (np.sqrt(test_score[0]), test_score[1]))\n",
    "model_loss(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_22 (LSTM)               (None, 1, 80)             29760     \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 1, 80)             0         \n",
      "_________________________________________________________________\n",
      "lstm_23 (LSTM)               (None, 1, 40)             19360     \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 1, 40)             0         \n",
      "_________________________________________________________________\n",
      "lstm_24 (LSTM)               (None, 20)                4880      \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 54,021\n",
      "Trainable params: 54,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "78/78 [==============================] - 1s 12ms/step - loss: 278534105559284480.0000\n",
      "Epoch 2/300\n",
      "78/78 [==============================] - 0s 249us/step - loss: 278534108202341280.0000\n",
      "Epoch 3/300\n",
      "78/78 [==============================] - 0s 209us/step - loss: 278534091022472096.0000\n",
      "Epoch 4/300\n",
      "78/78 [==============================] - 0s 267us/step - loss: 278534097630114080.0000\n",
      "Epoch 5/300\n",
      "78/78 [==============================] - 0s 254us/step - loss: 278534083093301728.0000\n",
      "Epoch 6/300\n",
      "78/78 [==============================] - 0s 279us/step - loss: 278534094987057312.0000\n",
      "Epoch 7/300\n",
      "78/78 [==============================] - 0s 224us/step - loss: 278534084414830112.0000\n",
      "Epoch 8/300\n",
      "78/78 [==============================] - 0s 365us/step - loss: 278534089700943712.0000\n",
      "Epoch 9/300\n",
      "78/78 [==============================] - 0s 327us/step - loss: 278534104237756128.0000\n",
      "Epoch 10/300\n",
      "78/78 [==============================] - 0s 250us/step - loss: 278534084414830112.0000\n",
      "Epoch 11/300\n",
      "78/78 [==============================] - 0s 253us/step - loss: 278534085736358528.0000\n",
      "Epoch 12/300\n",
      "78/78 [==============================] - 0s 234us/step - loss: 278534098951642496.0000\n",
      "Epoch 13/300\n",
      "78/78 [==============================] - 0s 310us/step - loss: 278534091022472096.0000\n",
      "Epoch 14/300\n",
      "78/78 [==============================] - 0s 189us/step - loss: 278534101594699296.0000\n",
      "Epoch 15/300\n",
      "78/78 [==============================] - 0s 199us/step - loss: 278534094987057312.0000\n",
      "Epoch 16/300\n",
      "78/78 [==============================] - 0s 208us/step - loss: 278534089700943712.0000\n",
      "Epoch 17/300\n",
      "78/78 [==============================] - 0s 215us/step - loss: 278534085736358528.0000\n",
      "Epoch 18/300\n",
      "78/78 [==============================] - 0s 204us/step - loss: 278534079128716544.0000\n",
      "Epoch 19/300\n",
      "78/78 [==============================] - 0s 237us/step - loss: 278534080450244896.0000\n",
      "Epoch 20/300\n",
      "78/78 [==============================] - 0s 227us/step - loss: 278534085736358528.0000\n",
      "Epoch 21/300\n",
      "78/78 [==============================] - 0s 208us/step - loss: 278534105559284480.0000\n",
      "Epoch 22/300\n",
      "78/78 [==============================] - 0s 260us/step - loss: 278534112166926496.0000\n",
      "Epoch 23/300\n",
      "78/78 [==============================] - 0s 219us/step - loss: 278534089700943712.0000\n",
      "Epoch 24/300\n",
      "78/78 [==============================] - 0s 239us/step - loss: 278534101594699296.0000\n",
      "Epoch 25/300\n",
      "78/78 [==============================] - 0s 206us/step - loss: 278534087057886944.0000\n",
      "Epoch 26/300\n",
      "78/78 [==============================] - 0s 228us/step - loss: 278534092344000512.0000\n",
      "Epoch 27/300\n",
      "78/78 [==============================] - 0s 245us/step - loss: 278534096308585728.0000\n",
      "Epoch 28/300\n",
      "78/78 [==============================] - 0s 215us/step - loss: 278534106880812896.0000\n",
      "Epoch 29/300\n",
      "78/78 [==============================] - 0s 227us/step - loss: 278534105559284480.0000\n",
      "Epoch 30/300\n",
      "78/78 [==============================] - 0s 221us/step - loss: 278534109523869696.0000\n",
      "Epoch 31/300\n",
      "78/78 [==============================] - 0s 210us/step - loss: 278534102916227712.0000\n",
      "Epoch 32/300\n",
      "78/78 [==============================] - 0s 221us/step - loss: 278534085736358528.0000\n",
      "Epoch 33/300\n",
      "78/78 [==============================] - 0s 204us/step - loss: 278534096308585728.0000\n",
      "Epoch 34/300\n",
      "78/78 [==============================] - 0s 200us/step - loss: 278534098951642496.0000\n",
      "Epoch 35/300\n",
      "78/78 [==============================] - 0s 231us/step - loss: 278534105559284480.0000\n",
      "Epoch 36/300\n",
      "78/78 [==============================] - 0s 238us/step - loss: 278534087057886944.0000\n",
      "Epoch 37/300\n",
      "78/78 [==============================] - 0s 246us/step - loss: 278534092344000512.0000\n",
      "Epoch 38/300\n",
      "78/78 [==============================] - 0s 266us/step - loss: 278534094987057312.0000\n",
      "Epoch 39/300\n",
      "78/78 [==============================] - 0s 247us/step - loss: 278534081771773312.0000\n",
      "Epoch 40/300\n",
      "78/78 [==============================] - 0s 234us/step - loss: 278534102916227712.0000\n",
      "Epoch 41/300\n",
      "78/78 [==============================] - 0s 199us/step - loss: 278534094987057312.0000\n",
      "Epoch 42/300\n",
      "78/78 [==============================] - 0s 268us/step - loss: 278534106880812896.0000\n",
      "Epoch 43/300\n",
      "78/78 [==============================] - 0s 224us/step - loss: 278534091022472096.0000\n",
      "Epoch 44/300\n",
      "78/78 [==============================] - 0s 206us/step - loss: 278534098951642496.0000\n",
      "Epoch 45/300\n",
      "78/78 [==============================] - 0s 217us/step - loss: 278534097630114080.0000\n",
      "Epoch 46/300\n",
      "78/78 [==============================] - 0s 229us/step - loss: 278534097630114080.0000\n",
      "Epoch 47/300\n",
      "78/78 [==============================] - 0s 200us/step - loss: 278534087057886944.0000\n",
      "Epoch 48/300\n",
      "78/78 [==============================] - 0s 229us/step - loss: 278534096308585728.0000\n",
      "Epoch 49/300\n",
      "78/78 [==============================] - 0s 195us/step - loss: 278534096308585728.0000\n",
      "Epoch 50/300\n",
      "78/78 [==============================] - 0s 188us/step - loss: 278534091022472096.0000\n",
      "Epoch 51/300\n",
      "78/78 [==============================] - 0s 185us/step - loss: 278534101594699296.0000\n",
      "Epoch 52/300\n",
      "78/78 [==============================] - 0s 204us/step - loss: 278534093665528928.0000\n",
      "Epoch 53/300\n",
      "78/78 [==============================] - 0s 199us/step - loss: 278534089700943712.0000\n",
      "Epoch 54/300\n",
      "78/78 [==============================] - 0s 285us/step - loss: 278534081771773312.0000\n",
      "Epoch 55/300\n",
      "78/78 [==============================] - 0s 188us/step - loss: 278534100273170912.0000\n",
      "Epoch 56/300\n",
      "78/78 [==============================] - 0s 212us/step - loss: 278534102916227712.0000\n",
      "Epoch 57/300\n",
      "78/78 [==============================] - 0s 195us/step - loss: 278534091022472096.0000\n",
      "Epoch 58/300\n",
      "78/78 [==============================] - 0s 256us/step - loss: 278534096308585728.0000\n",
      "Epoch 59/300\n",
      "78/78 [==============================] - 0s 213us/step - loss: 278534108202341280.0000\n",
      "Epoch 60/300\n",
      "78/78 [==============================] - 0s 227us/step - loss: 278534106880812896.0000\n",
      "Epoch 61/300\n",
      "78/78 [==============================] - 0s 214us/step - loss: 278534101594699296.0000\n",
      "Epoch 62/300\n",
      "78/78 [==============================] - 0s 222us/step - loss: 278534097630114080.0000\n",
      "Epoch 63/300\n",
      "78/78 [==============================] - 0s 230us/step - loss: 278534096308585728.0000\n",
      "Epoch 64/300\n",
      "78/78 [==============================] - 0s 237us/step - loss: 278534102916227712.0000\n",
      "Epoch 65/300\n",
      "78/78 [==============================] - 0s 176us/step - loss: 278534088379415296.0000\n",
      "Epoch 66/300\n",
      "78/78 [==============================] - 0s 190us/step - loss: 278534100273170912.0000\n",
      "Epoch 67/300\n",
      "78/78 [==============================] - 0s 200us/step - loss: 278534102916227712.0000\n",
      "Epoch 68/300\n",
      "78/78 [==============================] - 0s 213us/step - loss: 278534108202341280.0000\n",
      "Epoch 69/300\n",
      "78/78 [==============================] - 0s 262us/step - loss: 278534104237756128.0000\n",
      "Epoch 70/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 [==============================] - 0s 205us/step - loss: 278534094987057312.0000\n",
      "Epoch 71/300\n",
      "78/78 [==============================] - 0s 302us/step - loss: 278534100273170912.0000\n",
      "Epoch 72/300\n",
      "78/78 [==============================] - 0s 178us/step - loss: 278534089700943712.0000\n",
      "Epoch 73/300\n",
      "78/78 [==============================] - 0s 308us/step - loss: 278534094987057312.0000\n",
      "Epoch 74/300\n",
      "78/78 [==============================] - 0s 186us/step - loss: 278534102916227712.0000\n",
      "Epoch 75/300\n",
      "78/78 [==============================] - 0s 171us/step - loss: 278534087057886944.0000\n",
      "Epoch 76/300\n",
      "78/78 [==============================] - 0s 179us/step - loss: 278534100273170912.0000\n",
      "Epoch 77/300\n",
      "78/78 [==============================] - 0s 181us/step - loss: 278534102916227712.0000\n",
      "Epoch 78/300\n",
      "78/78 [==============================] - 0s 249us/step - loss: 278534088379415296.0000\n",
      "Epoch 79/300\n",
      "78/78 [==============================] - 0s 174us/step - loss: 278534081771773312.0000\n",
      "Epoch 80/300\n",
      "78/78 [==============================] - 0s 154us/step - loss: 278534077807188128.0000\n",
      "Epoch 81/300\n",
      "78/78 [==============================] - 0s 179us/step - loss: 278534096308585728.0000\n",
      "Epoch 82/300\n",
      "78/78 [==============================] - 0s 164us/step - loss: 278534098951642496.0000\n",
      "Epoch 83/300\n",
      "78/78 [==============================] - 0s 170us/step - loss: 278534105559284480.0000\n",
      "Epoch 84/300\n",
      "78/78 [==============================] - 0s 169us/step - loss: 278534094987057312.0000\n",
      "Epoch 85/300\n",
      "78/78 [==============================] - 0s 190us/step - loss: 278534106880812896.0000\n",
      "Epoch 86/300\n",
      "78/78 [==============================] - 0s 249us/step - loss: 278534092344000512.0000\n",
      "Epoch 87/300\n",
      "78/78 [==============================] - 0s 308us/step - loss: 278534091022472096.0000\n",
      "Epoch 88/300\n",
      "78/78 [==============================] - 0s 303us/step - loss: 278534088379415296.0000\n",
      "Epoch 89/300\n",
      "78/78 [==============================] - 0s 256us/step - loss: 278534106880812896.0000\n",
      "Epoch 90/300\n",
      "78/78 [==============================] - 0s 239us/step - loss: 278534093665528928.0000\n",
      "Epoch 91/300\n",
      "78/78 [==============================] - 0s 227us/step - loss: 278534102916227712.0000\n",
      "Epoch 92/300\n",
      "78/78 [==============================] - 0s 235us/step - loss: 278534112166926496.0000\n",
      "Epoch 93/300\n",
      "78/78 [==============================] - 0s 261us/step - loss: 278534105559284480.0000\n",
      "Epoch 94/300\n",
      "78/78 [==============================] - 0s 247us/step - loss: 278534097630114080.0000\n",
      "Epoch 95/300\n",
      "78/78 [==============================] - 0s 275us/step - loss: 278534085736358528.0000\n",
      "Epoch 96/300\n",
      "78/78 [==============================] - 0s 220us/step - loss: 278534097630114080.0000\n",
      "Epoch 97/300\n",
      "78/78 [==============================] - 0s 209us/step - loss: 278534094987057312.0000\n",
      "Epoch 98/300\n",
      "78/78 [==============================] - 0s 180us/step - loss: 278534089700943712.0000\n",
      "Epoch 99/300\n",
      "78/78 [==============================] - 0s 185us/step - loss: 278534091022472096.0000\n",
      "Epoch 100/300\n",
      "78/78 [==============================] - 0s 200us/step - loss: 278534110845398112.0000\n",
      "Epoch 101/300\n",
      "78/78 [==============================] - 0s 207us/step - loss: 278534088379415296.0000\n",
      "Epoch 102/300\n",
      "78/78 [==============================] - 0s 183us/step - loss: 278534102916227712.0000\n",
      "Epoch 103/300\n",
      "78/78 [==============================] - 0s 212us/step - loss: 278534091022472096.0000\n",
      "Epoch 104/300\n",
      "78/78 [==============================] - 0s 212us/step - loss: 278534092344000512.0000\n",
      "Epoch 105/300\n",
      "78/78 [==============================] - 0s 186us/step - loss: 278534094987057312.0000\n",
      "Epoch 106/300\n",
      "78/78 [==============================] - 0s 204us/step - loss: 278534097630114080.0000\n",
      "Epoch 107/300\n",
      "78/78 [==============================] - 0s 223us/step - loss: 278534098951642496.0000\n",
      "Epoch 108/300\n",
      "78/78 [==============================] - 0s 191us/step - loss: 278534110845398112.0000\n",
      "Epoch 109/300\n",
      "78/78 [==============================] - 0s 256us/step - loss: 278534100273170912.0000\n",
      "Epoch 110/300\n",
      "78/78 [==============================] - 0s 249us/step - loss: 278534133311380864.0000\n",
      "Epoch 111/300\n",
      "78/78 [==============================] - 0s 185us/step - loss: 278534097630114080.0000\n",
      "Epoch 112/300\n",
      "78/78 [==============================] - 0s 190us/step - loss: 278534105559284480.0000\n",
      "Epoch 113/300\n",
      "78/78 [==============================] - 0s 185us/step - loss: 278534087057886944.0000\n",
      "Epoch 114/300\n",
      "78/78 [==============================] - 0s 200us/step - loss: 278534108202341280.0000\n",
      "Epoch 115/300\n",
      "78/78 [==============================] - 0s 199us/step - loss: 278534080450244896.0000\n",
      "Epoch 116/300\n",
      "78/78 [==============================] - 0s 211us/step - loss: 278534087057886944.0000\n",
      "Epoch 117/300\n",
      "78/78 [==============================] - 0s 204us/step - loss: 278534112166926496.0000\n",
      "Epoch 118/300\n",
      "78/78 [==============================] - 0s 255us/step - loss: 278534094987057312.0000\n",
      "Epoch 119/300\n",
      "78/78 [==============================] - 0s 215us/step - loss: 278534091022472096.0000\n",
      "Epoch 120/300\n",
      "78/78 [==============================] - 0s 234us/step - loss: 278534108202341280.0000\n",
      "Epoch 121/300\n",
      "78/78 [==============================] - 0s 276us/step - loss: 278534092344000512.0000\n",
      "Epoch 122/300\n",
      "78/78 [==============================] - 0s 203us/step - loss: 278534081771773312.0000\n",
      "Epoch 123/300\n",
      "78/78 [==============================] - 0s 216us/step - loss: 278534085736358528.0000\n",
      "Epoch 124/300\n",
      "78/78 [==============================] - 0s 201us/step - loss: 278534102916227712.0000\n",
      "Epoch 125/300\n",
      "78/78 [==============================] - 0s 213us/step - loss: 278534092344000512.0000\n",
      "Epoch 126/300\n",
      "78/78 [==============================] - 0s 205us/step - loss: 278534104237756128.0000\n",
      "Epoch 127/300\n",
      "78/78 [==============================] - 0s 204us/step - loss: 278534092344000512.0000\n",
      "Epoch 128/300\n",
      "78/78 [==============================] - 0s 210us/step - loss: 278534093665528928.0000\n",
      "Epoch 129/300\n",
      "78/78 [==============================] - 0s 237us/step - loss: 278534117453040096.0000\n",
      "Epoch 130/300\n",
      "78/78 [==============================] - 0s 201us/step - loss: 278534084414830112.0000\n",
      "Epoch 131/300\n",
      "78/78 [==============================] - 0s 189us/step - loss: 278534097630114080.0000\n",
      "Epoch 132/300\n",
      "78/78 [==============================] - 0s 161us/step - loss: 278534097630114080.0000\n",
      "Epoch 133/300\n",
      "78/78 [==============================] - 0s 179us/step - loss: 278534088379415296.0000\n",
      "Epoch 134/300\n",
      "78/78 [==============================] - 0s 205us/step - loss: 278534114809983264.0000\n",
      "Epoch 135/300\n",
      "78/78 [==============================] - 0s 189us/step - loss: 278534096308585728.0000\n",
      "Epoch 136/300\n",
      "78/78 [==============================] - 0s 182us/step - loss: 278534096308585728.0000\n",
      "Epoch 137/300\n",
      "78/78 [==============================] - 0s 214us/step - loss: 278534097630114080.0000\n",
      "Epoch 138/300\n",
      "78/78 [==============================] - 0s 202us/step - loss: 278534091022472096.0000\n",
      "Epoch 139/300\n",
      "78/78 [==============================] - 0s 185us/step - loss: 278534087057886944.0000\n",
      "Epoch 140/300\n",
      "78/78 [==============================] - 0s 213us/step - loss: 278534092344000512.0000\n",
      "Epoch 141/300\n",
      "78/78 [==============================] - 0s 192us/step - loss: 278534089700943712.0000\n",
      "Epoch 142/300\n",
      "78/78 [==============================] - 0s 188us/step - loss: 278534105559284480.0000\n",
      "Epoch 143/300\n",
      "78/78 [==============================] - 0s 188us/step - loss: 278534094987057312.0000\n",
      "Epoch 144/300\n",
      "78/78 [==============================] - 0s 188us/step - loss: 278534102916227712.0000\n",
      "Epoch 145/300\n",
      "78/78 [==============================] - 0s 206us/step - loss: 278534096308585728.0000\n",
      "Epoch 146/300\n",
      "78/78 [==============================] - 0s 207us/step - loss: 278534117453040096.0000\n",
      "Epoch 147/300\n",
      "78/78 [==============================] - 0s 197us/step - loss: 278534110845398112.0000\n",
      "Epoch 148/300\n",
      "78/78 [==============================] - 0s 214us/step - loss: 278534094987057312.0000\n",
      "Epoch 149/300\n",
      "78/78 [==============================] - 0s 206us/step - loss: 278534098951642496.0000\n",
      "Epoch 150/300\n",
      "78/78 [==============================] - 0s 224us/step - loss: 278534096308585728.0000\n",
      "Epoch 151/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 [==============================] - 0s 186us/step - loss: 278534094987057312.0000\n",
      "Epoch 152/300\n",
      "78/78 [==============================] - 0s 184us/step - loss: 278534105559284480.0000\n",
      "Epoch 153/300\n",
      "78/78 [==============================] - 0s 230us/step - loss: 278534085736358528.0000\n",
      "Epoch 154/300\n",
      "78/78 [==============================] - 0s 200us/step - loss: 278534089700943712.0000\n",
      "Epoch 155/300\n",
      "78/78 [==============================] - 0s 204us/step - loss: 278534089700943712.0000\n",
      "Epoch 156/300\n",
      "78/78 [==============================] - 0s 219us/step - loss: 278534098951642496.0000\n",
      "Epoch 157/300\n",
      "78/78 [==============================] - 0s 186us/step - loss: 278534100273170912.0000\n",
      "Epoch 158/300\n",
      "78/78 [==============================] - 0s 207us/step - loss: 278534089700943712.0000\n",
      "Epoch 159/300\n",
      "78/78 [==============================] - 0s 193us/step - loss: 278534094987057312.0000\n",
      "Epoch 160/300\n",
      "78/78 [==============================] - 0s 201us/step - loss: 278534098951642496.0000\n",
      "Epoch 161/300\n",
      "78/78 [==============================] - 0s 225us/step - loss: 278534083093301728.0000\n",
      "Epoch 162/300\n",
      "78/78 [==============================] - 0s 211us/step - loss: 278534102916227712.0000\n",
      "Epoch 163/300\n",
      "78/78 [==============================] - 0s 194us/step - loss: 278534096308585728.0000\n",
      "Epoch 164/300\n",
      "78/78 [==============================] - 0s 194us/step - loss: 278534100273170912.0000\n",
      "Epoch 165/300\n",
      "78/78 [==============================] - 0s 181us/step - loss: 278534101594699296.0000\n",
      "Epoch 166/300\n",
      "78/78 [==============================] - 0s 183us/step - loss: 278534080450244896.0000\n",
      "Epoch 167/300\n",
      "78/78 [==============================] - 0s 171us/step - loss: 278534092344000512.0000\n",
      "Epoch 168/300\n",
      "78/78 [==============================] - 0s 169us/step - loss: 278534109523869696.0000\n",
      "Epoch 169/300\n",
      "78/78 [==============================] - 0s 169us/step - loss: 278534098951642496.0000\n",
      "Epoch 170/300\n",
      "78/78 [==============================] - 0s 171us/step - loss: 278534084414830112.0000\n",
      "Epoch 171/300\n",
      "78/78 [==============================] - 0s 161us/step - loss: 278534109523869696.0000\n",
      "Epoch 172/300\n",
      "78/78 [==============================] - 0s 174us/step - loss: 278534088379415296.0000\n",
      "Epoch 173/300\n",
      "78/78 [==============================] - 0s 177us/step - loss: 278534109523869696.0000\n",
      "Epoch 174/300\n",
      "78/78 [==============================] - 0s 197us/step - loss: 278534120096096896.0000\n",
      "Epoch 175/300\n",
      "78/78 [==============================] - 0s 188us/step - loss: 278534080450244896.0000\n",
      "Epoch 176/300\n",
      "78/78 [==============================] - 0s 176us/step - loss: 278534096308585728.0000\n",
      "Epoch 177/300\n",
      "78/78 [==============================] - 0s 163us/step - loss: 278534105559284480.0000\n",
      "Epoch 178/300\n",
      "78/78 [==============================] - 0s 171us/step - loss: 278534105559284480.0000\n",
      "Epoch 179/300\n",
      "78/78 [==============================] - 0s 186us/step - loss: 278534102916227712.0000\n",
      "Epoch 180/300\n",
      "78/78 [==============================] - 0s 162us/step - loss: 278534105559284480.0000\n",
      "Epoch 181/300\n",
      "78/78 [==============================] - 0s 163us/step - loss: 278534102916227712.0000\n",
      "Epoch 182/300\n",
      "78/78 [==============================] - 0s 195us/step - loss: 278534098951642496.0000\n",
      "Epoch 183/300\n",
      "78/78 [==============================] - 0s 188us/step - loss: 278534094987057312.0000\n",
      "Epoch 184/300\n",
      "78/78 [==============================] - 0s 206us/step - loss: 278534092344000512.0000\n",
      "Epoch 185/300\n",
      "78/78 [==============================] - 0s 207us/step - loss: 278534094987057312.0000\n",
      "Epoch 186/300\n",
      "78/78 [==============================] - 0s 190us/step - loss: 278534105559284480.0000\n",
      "Epoch 187/300\n",
      "78/78 [==============================] - 0s 188us/step - loss: 278534097630114080.0000\n",
      "Epoch 188/300\n",
      "78/78 [==============================] - 0s 190us/step - loss: 278534112166926496.0000\n",
      "Epoch 189/300\n",
      "78/78 [==============================] - 0s 167us/step - loss: 278534084414830112.0000\n",
      "Epoch 190/300\n",
      "78/78 [==============================] - 0s 186us/step - loss: 278534098951642496.0000\n",
      "Epoch 191/300\n",
      "78/78 [==============================] - 0s 188us/step - loss: 278534114809983264.0000\n",
      "Epoch 192/300\n",
      "78/78 [==============================] - 0s 200us/step - loss: 278534094987057312.0000\n",
      "Epoch 193/300\n",
      "78/78 [==============================] - 0s 195us/step - loss: 278534098951642496.0000\n",
      "Epoch 194/300\n",
      "78/78 [==============================] - 0s 158us/step - loss: 278534093665528928.0000\n",
      "Epoch 195/300\n",
      "78/78 [==============================] - 0s 180us/step - loss: 278534097630114080.0000\n",
      "Epoch 196/300\n",
      "78/78 [==============================] - 0s 182us/step - loss: 278534106880812896.0000\n",
      "Epoch 197/300\n",
      "78/78 [==============================] - 0s 179us/step - loss: 278534081771773312.0000\n",
      "Epoch 198/300\n",
      "78/78 [==============================] - 0s 222us/step - loss: 278534097630114080.0000\n",
      "Epoch 199/300\n",
      "78/78 [==============================] - 0s 221us/step - loss: 278534083093301728.0000\n",
      "Epoch 200/300\n",
      "78/78 [==============================] - 0s 180us/step - loss: 278534098951642496.0000\n",
      "Epoch 201/300\n",
      "78/78 [==============================] - 0s 216us/step - loss: 278534100273170912.0000\n",
      "Epoch 202/300\n",
      "78/78 [==============================] - 0s 241us/step - loss: 278534110845398112.0000\n",
      "Epoch 203/300\n",
      "78/78 [==============================] - 0s 221us/step - loss: 278534084414830112.0000\n",
      "Epoch 204/300\n",
      "78/78 [==============================] - 0s 188us/step - loss: 278534088379415296.0000\n",
      "Epoch 205/300\n",
      "78/78 [==============================] - 0s 186us/step - loss: 278534105559284480.0000\n",
      "Epoch 206/300\n",
      "78/78 [==============================] - 0s 201us/step - loss: 278534105559284480.0000\n",
      "Epoch 207/300\n",
      "78/78 [==============================] - 0s 214us/step - loss: 278534102916227712.0000\n",
      "Epoch 208/300\n",
      "78/78 [==============================] - 0s 211us/step - loss: 278534110845398112.0000\n",
      "Epoch 209/300\n",
      "78/78 [==============================] - 0s 209us/step - loss: 278534084414830112.0000\n",
      "Epoch 210/300\n",
      "78/78 [==============================] - 0s 202us/step - loss: 278534109523869696.0000\n",
      "Epoch 211/300\n",
      "78/78 [==============================] - 0s 225us/step - loss: 278534087057886944.0000\n",
      "Epoch 212/300\n",
      "78/78 [==============================] - 0s 212us/step - loss: 278534088379415296.0000\n",
      "Epoch 213/300\n",
      "78/78 [==============================] - 0s 208us/step - loss: 278534110845398112.0000\n",
      "Epoch 214/300\n",
      "78/78 [==============================] - 0s 213us/step - loss: 278534101594699296.0000\n",
      "Epoch 215/300\n",
      "78/78 [==============================] - 0s 236us/step - loss: 278534091022472096.0000\n",
      "Epoch 216/300\n",
      "78/78 [==============================] - 0s 439us/step - loss: 278534098951642496.0000\n",
      "Epoch 217/300\n",
      "78/78 [==============================] - 0s 267us/step - loss: 278534088379415296.0000\n",
      "Epoch 218/300\n",
      "78/78 [==============================] - 0s 164us/step - loss: 278534102916227712.0000\n",
      "Epoch 219/300\n",
      "78/78 [==============================] - 0s 197us/step - loss: 278534094987057312.0000\n",
      "Epoch 220/300\n",
      "78/78 [==============================] - 0s 173us/step - loss: 278534109523869696.0000\n",
      "Epoch 221/300\n",
      "78/78 [==============================] - 0s 166us/step - loss: 278534100273170912.0000\n",
      "Epoch 222/300\n",
      "78/78 [==============================] - 0s 173us/step - loss: 278534083093301728.0000\n",
      "Epoch 223/300\n",
      "78/78 [==============================] - 0s 173us/step - loss: 278534088379415296.0000\n",
      "Epoch 224/300\n",
      "78/78 [==============================] - 0s 213us/step - loss: 278534101594699296.0000\n",
      "Epoch 225/300\n",
      "78/78 [==============================] - 0s 187us/step - loss: 278534094987057312.0000\n",
      "Epoch 226/300\n",
      "78/78 [==============================] - 0s 169us/step - loss: 278534097630114080.0000\n",
      "Epoch 227/300\n",
      "78/78 [==============================] - 0s 176us/step - loss: 278534091022472096.0000\n",
      "Epoch 228/300\n",
      "78/78 [==============================] - 0s 231us/step - loss: 278534105559284480.0000\n",
      "Epoch 229/300\n",
      "78/78 [==============================] - 0s 191us/step - loss: 278534101594699296.0000\n",
      "Epoch 230/300\n",
      "78/78 [==============================] - 0s 192us/step - loss: 278534102916227712.0000\n",
      "Epoch 231/300\n",
      "78/78 [==============================] - 0s 190us/step - loss: 278534097630114080.0000\n",
      "Epoch 232/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 [==============================] - 0s 165us/step - loss: 278534085736358528.0000\n",
      "Epoch 233/300\n",
      "78/78 [==============================] - 0s 174us/step - loss: 278534108202341280.0000\n",
      "Epoch 234/300\n",
      "78/78 [==============================] - 0s 208us/step - loss: 278534093665528928.0000\n",
      "Epoch 235/300\n",
      "78/78 [==============================] - 0s 186us/step - loss: 278534106880812896.0000\n",
      "Epoch 236/300\n",
      "78/78 [==============================] - 0s 170us/step - loss: 278534085736358528.0000\n",
      "Epoch 237/300\n",
      "78/78 [==============================] - 0s 178us/step - loss: 278534094987057312.0000\n",
      "Epoch 238/300\n",
      "78/78 [==============================] - ETA: 0s - loss: 284538525343285248.00 - 0s 173us/step - loss: 278534096308585728.0000\n",
      "Epoch 239/300\n",
      "78/78 [==============================] - 0s 213us/step - loss: 278534080450244896.0000\n",
      "Epoch 240/300\n",
      "78/78 [==============================] - 0s 211us/step - loss: 278534079128716544.0000\n",
      "Epoch 241/300\n",
      "78/78 [==============================] - 0s 213us/step - loss: 278534093665528928.0000\n",
      "Epoch 242/300\n",
      "78/78 [==============================] - 0s 223us/step - loss: 278534077807188128.0000\n",
      "Epoch 243/300\n",
      "78/78 [==============================] - 0s 219us/step - loss: 278534088379415296.0000\n",
      "Epoch 244/300\n",
      "78/78 [==============================] - 0s 196us/step - loss: 278534091022472096.0000\n",
      "Epoch 245/300\n",
      "78/78 [==============================] - 0s 207us/step - loss: 278534091022472096.0000\n",
      "Epoch 246/300\n",
      "78/78 [==============================] - 0s 210us/step - loss: 278534091022472096.0000\n",
      "Epoch 247/300\n",
      "78/78 [==============================] - 0s 191us/step - loss: 278534092344000512.0000\n",
      "Epoch 248/300\n",
      "78/78 [==============================] - 0s 176us/step - loss: 278534092344000512.0000\n",
      "Epoch 249/300\n",
      "78/78 [==============================] - 0s 162us/step - loss: 278534105559284480.0000\n",
      "Epoch 250/300\n",
      "78/78 [==============================] - 0s 159us/step - loss: 278534104237756128.0000\n",
      "Epoch 251/300\n",
      "78/78 [==============================] - 0s 155us/step - loss: 278534096308585728.0000\n",
      "Epoch 252/300\n",
      "78/78 [==============================] - 0s 183us/step - loss: 278534110845398112.0000\n",
      "Epoch 253/300\n",
      "78/78 [==============================] - 0s 195us/step - loss: 278534098951642496.0000\n",
      "Epoch 254/300\n",
      "78/78 [==============================] - 0s 185us/step - loss: 278534102916227712.0000\n",
      "Epoch 255/300\n",
      "78/78 [==============================] - 0s 200us/step - loss: 278534092344000512.0000\n",
      "Epoch 256/300\n",
      "78/78 [==============================] - 0s 201us/step - loss: 278534102916227712.0000\n",
      "Epoch 257/300\n",
      "78/78 [==============================] - 0s 203us/step - loss: 278534083093301728.0000\n",
      "Epoch 258/300\n",
      "78/78 [==============================] - 0s 194us/step - loss: 278534106880812896.0000\n",
      "Epoch 259/300\n",
      "78/78 [==============================] - 0s 212us/step - loss: 278534087057886944.0000\n",
      "Epoch 260/300\n",
      "78/78 [==============================] - 0s 193us/step - loss: 278534113488454912.0000\n",
      "Epoch 261/300\n",
      "78/78 [==============================] - 0s 190us/step - loss: 278534089700943712.0000\n",
      "Epoch 262/300\n",
      "78/78 [==============================] - 0s 189us/step - loss: 278534091022472096.0000\n",
      "Epoch 263/300\n",
      "78/78 [==============================] - 0s 236us/step - loss: 278534093665528928.0000\n",
      "Epoch 264/300\n",
      "78/78 [==============================] - 0s 232us/step - loss: 278534104237756128.0000\n",
      "Epoch 265/300\n",
      "78/78 [==============================] - 0s 177us/step - loss: 278534097630114080.0000\n",
      "Epoch 266/300\n",
      "78/78 [==============================] - 0s 183us/step - loss: 278534091022472096.0000\n",
      "Epoch 267/300\n",
      "78/78 [==============================] - 0s 205us/step - loss: 278534102916227712.0000\n",
      "Epoch 268/300\n",
      "78/78 [==============================] - 0s 197us/step - loss: 278534101594699296.0000\n",
      "Epoch 269/300\n",
      "78/78 [==============================] - 0s 180us/step - loss: 278534104237756128.0000\n",
      "Epoch 270/300\n",
      "78/78 [==============================] - 0s 183us/step - loss: 278534083093301728.0000\n",
      "Epoch 271/300\n",
      "78/78 [==============================] - 0s 160us/step - loss: 278534079128716544.0000\n",
      "Epoch 272/300\n",
      "78/78 [==============================] - 0s 168us/step - loss: 278534104237756128.0000\n",
      "Epoch 273/300\n",
      "78/78 [==============================] - 0s 165us/step - loss: 278534105559284480.0000\n",
      "Epoch 274/300\n",
      "78/78 [==============================] - 0s 156us/step - loss: 278534093665528928.0000\n",
      "Epoch 275/300\n",
      "78/78 [==============================] - 0s 185us/step - loss: 278534096308585728.0000\n",
      "Epoch 276/300\n",
      "78/78 [==============================] - 0s 183us/step - loss: 278534122739153664.0000\n",
      "Epoch 277/300\n",
      "78/78 [==============================] - 0s 186us/step - loss: 278534089700943712.0000\n",
      "Epoch 278/300\n",
      "78/78 [==============================] - 0s 189us/step - loss: 278534072521074528.0000\n",
      "Epoch 279/300\n",
      "78/78 [==============================] - 0s 182us/step - loss: 278534080450244896.0000\n",
      "Epoch 280/300\n",
      "78/78 [==============================] - 0s 178us/step - loss: 278534094987057312.0000\n",
      "Epoch 281/300\n",
      "78/78 [==============================] - 0s 172us/step - loss: 278534093665528928.0000\n",
      "Epoch 282/300\n",
      "78/78 [==============================] - 0s 184us/step - loss: 278534094987057312.0000\n",
      "Epoch 283/300\n",
      "78/78 [==============================] - 0s 193us/step - loss: 278534104237756128.0000\n",
      "Epoch 284/300\n",
      "78/78 [==============================] - 0s 174us/step - loss: 278534083093301728.0000\n",
      "Epoch 285/300\n",
      "78/78 [==============================] - 0s 218us/step - loss: 278534094987057312.0000\n",
      "Epoch 286/300\n",
      "78/78 [==============================] - 0s 222us/step - loss: 278534113488454912.0000\n",
      "Epoch 287/300\n",
      "78/78 [==============================] - 0s 195us/step - loss: 278534098951642496.0000\n",
      "Epoch 288/300\n",
      "78/78 [==============================] - 0s 187us/step - loss: 278534094987057312.0000\n",
      "Epoch 289/300\n",
      "78/78 [==============================] - 0s 185us/step - loss: 278534088379415296.0000\n",
      "Epoch 290/300\n",
      "78/78 [==============================] - 0s 166us/step - loss: 278534098951642496.0000\n",
      "Epoch 291/300\n",
      "78/78 [==============================] - 0s 209us/step - loss: 278534084414830112.0000\n",
      "Epoch 292/300\n",
      "78/78 [==============================] - 0s 243us/step - loss: 278534100273170912.0000\n",
      "Epoch 293/300\n",
      "78/78 [==============================] - 0s 221us/step - loss: 278534106880812896.0000\n",
      "Epoch 294/300\n",
      "78/78 [==============================] - 0s 225us/step - loss: 278534092344000512.0000\n",
      "Epoch 295/300\n",
      "78/78 [==============================] - 0s 216us/step - loss: 278534105559284480.0000\n",
      "Epoch 296/300\n",
      "78/78 [==============================] - 0s 232us/step - loss: 278534093665528928.0000\n",
      "Epoch 297/300\n",
      "78/78 [==============================] - 0s 224us/step - loss: 278534097630114080.0000\n",
      "Epoch 298/300\n",
      "78/78 [==============================] - 0s 209us/step - loss: 278534100273170912.0000\n",
      "Epoch 299/300\n",
      "78/78 [==============================] - 0s 201us/step - loss: 278534116131511680.0000\n",
      "Epoch 300/300\n",
      "78/78 [==============================] - 0s 191us/step - loss: 278534083093301728.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f91d95d12e0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trainX=trainX.reshape(trainX.shape[0], 1, trainX.shape[1]) \n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(80,activation=\"sigmoid\",return_sequences=True, input_shape=(trainX.shape[1:])))\n",
    "lstm_model.add(Dropout(0.15))\n",
    "lstm_model.add(LSTM(40,activation=\"sigmoid\",return_sequences=True))\n",
    "lstm_model.add(Dropout(0.15))\n",
    "lstm_model.add(LSTM(20,activation=\"sigmoid\",return_sequences=False))\n",
    "lstm_model.add(Dropout(0.15))\n",
    "lstm_model.add(Dense(1))\n",
    "lstm_model.summary()\n",
    "\n",
    "lstm_model.compile(optimizer=\"adam\",loss=\"MSE\")\n",
    "lstm_model.fit(trainX, trainY, epochs=300, batch_size=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score of LSTM model =  -49.95090674118526\n"
     ]
    }
   ],
   "source": [
    "#testX=testX.reshape(testX.shape[0], 1, testX.shape[1]) \n",
    "\n",
    "lstm_predictions = lstm_model.predict(testX)\n",
    "\n",
    "lstm_score = r2_score(testY, lstm_predictions)\n",
    "print(\"R^2 Score of LSTM model = \",lstm_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(test, predicted, title):\n",
    "    plt.figure(figsize=(16,4))\n",
    "    plt.plot(test, color='blue',label='Actual power consumption data')\n",
    "    plt.plot(predicted, alpha=0.7, color='orange',label='Predicted power consumption data')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Normalized power consumption scale')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions(testY, lstm_predictions, \"Predictions made by LSTM model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc4cabee3265ec5cdba863c8d7651e06fcceb9563f0612ecf7571a8baa334d71"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
